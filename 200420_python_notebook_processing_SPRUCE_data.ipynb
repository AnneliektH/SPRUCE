{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook for producing vOTU table from raw sequencing reads as in \"Minnesota peat viromes reveal terrestrial and aquatic niche partitioning for local and global viral populations\"\n",
    "Anneliek ter Horst\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## QC reads with trimmomatic\n",
    "\n",
    "- Trim the reads with trimmomatic, following Roux et al., 2017\n",
    "- minimum base quality threshold of 30 evaluated on sliding windows of 4 bases, and minimum read length of 50\n",
    "- As a rule of thumb newer libraries will use TruSeq3, but this really depends on your service provider.\n",
    "- http://www.usadellab.org/cms/?page=trimmomatic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load modules\n",
    "module load java\n",
    "module load trimmomatic\n",
    "\n",
    "# use trimmomatic to trim adapters, make sure you have the correct adapterfile\n",
    "trimmomatic PE -threads 8 -phred33 $1 ${1%%_1*}_2.fastq.gz \\\n",
    "../trimmed/${1%%_1*}_1_trimmed.fq.gz unpaired/${1%%_1*}_1_Unpaired.fq.gz \\\n",
    "../trimmed/${1%%_1*}_2_trimmed.fq.gz unpaired/${1%%_1*}_2_Unpaired.fq.gz \\\n",
    "ILLUMINACLIP:/adapters/TruSeq3-PE.fa:2:30:10 \\\n",
    "SLIDINGWINDOW:4:30 MINLEN:50 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove PhiX174 with bbduk\n",
    "- Phix174 is a phage sequence that is used to aid in sequencing. This needs to be removed from our sequencing data\n",
    "- https://jgi.doe.gov/data-and-tools/bbtools/bb-tools-user-guide/bbduk-guide/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load module\n",
    "module load java\n",
    "module load bbmap\n",
    "\n",
    "# use bbduk to remove phix contamination\n",
    "bbduk.sh in1=$1 in2=${1%%_1*}_2_trimmed.fq.gz \\\n",
    "out1=../remove_phix/${1%%_1*}_1_trimmed_bbduk.fq.gz out2=../remove_phix/${1%%_1*}_2_trimmed_bbduk.fq.gz \\\n",
    "ref=/bbduk/phix_genome.fa k=31 \\\n",
    "hdist=1 stats=../remove_phix/${1%%}_stats.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assembly with MEGAHIT\n",
    "- Assemble viromes following Roux et al., 2017:\n",
    "\n",
    "- Assemble total soil metagenomes following Sczyra et al., 2017 and Vollmer et al., 2017 \n",
    "\n",
    "- https://github.com/voutcn/megahit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MEGAHIT for single sample assembly\n",
    "megahit -1 $1 -2 ${1%%1_R1*}1_R2_trimmed_bbduk.fq.gz -o ../assemblies_persample/${1%%_R1*} \\\n",
    "--out-prefix ${1%%_R1*} --min-contig-len 10000 --continue\n",
    "\n",
    "\n",
    "# Co-assemly viromes\n",
    "READ1=*_R1_*\n",
    "READ2=*_R2_*\n",
    "READ1s=`ls $READ1 | python -c 'import sys; print \",\".join([x.strip() for x in sys.stdin.readlines()])'`\n",
    "READ2s=`ls $READ2 | python -c 'import sys; print \",\".join([x.strip() for x in sys.stdin.readlines()])'`\n",
    "\n",
    "megahit -1 ${READ1s} -2 ${READ2s} -o co_assembly_virome -t 64 -m 480e9 --mem-flag=2 \\\n",
    "--k-min 27 --out-prefix co_assembly_virome \\\n",
    "--min-contig-len 10000 --presets meta-large\n",
    "\n",
    "\n",
    "# total soil metagenomes\n",
    "megahit -1 $1 -2 ${1%%1_R1*}1_R2_trimmed_bbduk.fq.gz -o ../bact_assemblies_persample/${1%%_R1*} \\\n",
    "--out-prefix ${1%%_R1*} --min-contig-len 2000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DeepVirfinder\n",
    "- Only contigs that have a virFinder score => 0.9 and p <0.05 Gregory et al., 2019\n",
    "- https://github.com/jessieren/DeepVirFinder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run dvf\n",
    "python dvf.py -i $1 -l 10000 -o ${1%%.fna*}_virfinder -c 8 \\\n",
    "\n",
    "# use custom script to parse dvf data\n",
    "python parse_DVF.py $f/*.txt ${f%%_virfinder*}_seq_names.txt\n",
    "\n",
    "\n",
    "# Pull those sequences from the fasta files\n",
    "for f in *.fa; do\n",
    "awk -F'>' 'NR==FNR{ids[$0]; next} NF>1{f=($2 in ids)} f' \\\n",
    "../virfinder/${f%%.fa*}_seq_names.txt $f \\\n",
    "> ../virfinder/${f%%contigs.fa*}virfinder.fa\n",
    "done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"parse_dvf.py, Anneliek ter Horst, September 2019\n",
    "This script filters the deepVirfinder output to only keep entries with a\n",
    "virfinder score > 0.9 and a pvalue < 0.05\"\"\"\n",
    "\n",
    "#! /usr/bin/env python \n",
    "# imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import sys\n",
    "\n",
    "\n",
    "# open the coverage table\n",
    "df = pd.read_csv(sys.argv[1] ,sep='\\t')\n",
    "\n",
    "df = df[df.score > 0.899]\n",
    "df = df[df.pvalue < 0.05]\n",
    "print(len(df))\n",
    "\n",
    "df = df.name\n",
    "df.to_csv(sys.argv[2], sep='\\t',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pull the sequences that have high enough score from the fasta file with all contigs\n",
    "for f in *.fna; do\n",
    "awk -F'>' 'NR==FNR{ids[$0]; next} NF>1{f=($2 in ids)} f' \\\n",
    "${f%%.fna*}_seq_names.txt $f > ${f%%contigs.fna*}virfinder.fa\n",
    "done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VirSorter\n",
    "- Use Virsorter as well to predict viral sequences\n",
    "- Take only virsorter sequences from category 1,2,4 and 5\n",
    "- https://github.com/simroux/VirSorter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run virsorter\n",
    "wrapper_phage_contigs_sorter_iPlant.pl -f $f -ncpu 8 \\\n",
    "--data-dir virsorter-data/ -db 1 --wdir ${f%%.contigs.fa*}_virsorter --virome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate all virSorter output in said categories\n",
    "for f in *_virsorter; do\n",
    "cat $f/Pr*/VIRSorter*cat-[1245].fasta >> $f/${f%%_virsorter*}_virsort_viralseqs.fa\n",
    "done\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Concatenate, rename and sort viral sequences with bbmap\n",
    "- concatenate findings from Virsorter and Virfinder\n",
    "- rename those sequences according to the dataset they came from\n",
    "- Sort on lenght so we can cluster after\n",
    "- https://jgi.doe.gov/data-and-tools/bbtools/bb-tools-user-guide/bbmap-guide/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate\n",
    "cat *_virsorter_seqs.fa *_virfinder.fa >> *_all_found_viral_seqs.fa\n",
    "\n",
    "# rename (with bbmap rename.sh)\n",
    "rename.sh in=*_all_found_viral_seqs.fa out=*_all_found_viral_rename.fa prefix=SPRUCE_virome_\n",
    "\n",
    "# sort on length (with bbmap sortbyname.sh)\n",
    "sortbyname.sh in=*_all_found_viral_rename.fa  out=all_viral_seqs.sorted.fa length descending"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering found sequences with CD-hit\n",
    "- Clustering sequences at 95% ANI over 85% of the sequence, relative to the shorter sequence, following Roux et al., 2019\n",
    "- https://github.com/weizhongli/cdhit/wiki/2.-Installation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use CD-hit to cluster found viral sequences\n",
    "cd-hit-est -i all_viral_seqs.sorted.fa -o all_viral_seqs.sorted.cluster.fa \\\n",
    "-c 0.95 -aS 0.85 -M 7000 -T 10\n",
    "\n",
    "# use CD-hit to cluster those viral sequences with PIGEON\n",
    "cd-hit-est-2d -i all_viral_seqs.sorted.cluster.fa \\\n",
    "-i2 PIGEONv1.0.sorted.fa \\\n",
    "-o unique_viral_seqs.fa \\\n",
    "-c 0.95 -aS 0.85 -M 20000 -T 10\n",
    "\n",
    "\n",
    "# make one database from PIGEON and the newly found unique viral sequences\n",
    "cat PIGEONv1.0.sorted.fa unique_viral_seqs.fa >> PIGEON_and_new_SPRUCE.fa\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mapping back with BBMAP and keep only contigs that are > 75% covered in lenght\n",
    "- Use bbmap to map back clean reads to PIGEON database to see what viruses can be recovered via read mapping\n",
    "- Do this at 90% nucleotide identity, following Roux et al., 2017 \n",
    "- Do this at 75% of the contig lenght, following Roux et al., 2017\n",
    "- https://jgi.doe.gov/data-and-tools/bbtools/bb-tools-user-guide/bbmap-guide/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# produce reference mapping file\n",
    "bbmap.sh ref=PIGEON_and_new_SPRUCE.fa\n",
    "\n",
    "# map reads to the reference file (paired end reads)\n",
    "bbmap.sh in1=$f in2=${f%%_R1_*}_R2_bbduk.fq.gz \\\n",
    "out=${f%%_R1_*}.sam \\\n",
    "minid=0.90 threads=25\n",
    "\n",
    "# make bam files from sam files\n",
    "samtools view -F 4 -bS file.sam | samtools sort > file_sortedIndexed.bam\n",
    "\n",
    "# index the bam file\n",
    "samtools index ${f%%R1_*}_sortedIndexed.bam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate coverage length using bedtools\n",
    "- calculate for each bam file the coverage length\n",
    "- bga makes sure that also regions with 0 coverage are reported\n",
    "- max reports all reads with depth >10 as 10. Is faster. \n",
    "- https://bedtools.readthedocs.io/en/latest/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run bedtools\n",
    "bedtools genomecov -ibam $f -bga -max 10 > ${f%.bam*}.tsv\n",
    "\n",
    "# use custom python script to filter out contigs that aren't covered > 75%\n",
    "for f in $(ls *.tsv); do python parse_bedtools.py $f ${f%%.tsv*}_under75.tsv ; done; \n",
    "\n",
    "# for each sample make a list of names that are not 75% covered\n",
    "for f in $(ls *75.tsv); do cat $f | cut -d$'\\t' -f2  >  ${f%%.tsv*}.txt ; done;\n",
    "\n",
    "# coverage table needs to be filtered based on the contigs that dont have 75% coverage for each sample\n",
    "# so make a csv with [header]=filename, thus will correspond to a name in the covtab\n",
    "for filename in $(ls *.txt)\n",
    "do\n",
    "\tsed \"1s/^/${filename} \\n/\" ${filename} > $filename.new \n",
    "\techo Done ${filename} \n",
    "done\n",
    "\n",
    "# move all these files into folders\n",
    "mkdir under_75_csv \n",
    "mv *75.tsv under_75_csv \n",
    "mkdir under_75_txt\n",
    "mv *75.txt under_75_txt\n",
    "mkdir under_75_name_added\n",
    "mv *.new under_75_name_added\n",
    "\n",
    "# make one giant file that contains what contig in which sample isn't covered 75%\n",
    "paste -d \"|\" *txt.new > ../SPRUCE_contigs_under75.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! /usr/bin/env python \n",
    "\n",
    "'''\n",
    "python parse_bedtools.py, Anneliek ter Horst, September 2019\n",
    "bedtools genomecov outputs a tsv that we will manipulate with pandas to \n",
    "obtain the average coverage per contig. If its more than 75%, we will discard the contig.\n",
    "'''\n",
    "\n",
    "# imports\n",
    "from __future__ import division\n",
    "import sys\n",
    "import pandas as pd\n",
    "\n",
    "# open the file to read with pandas\n",
    "df = pd.read_csv(sys.argv[1],sep='\\t', header=None)\n",
    "\n",
    "# add column names so we know what we are looking at\n",
    "df.columns = ['sequence', 'start_pos', 'stop_pos' , 'coverage']\n",
    "\n",
    "# Add a column with the number of nucleotides that share the same coverage\n",
    "df['num_of_nucl'] = df['stop_pos'] - df['start_pos']\n",
    "\n",
    "# for each sequence, add a extra column witht the sequences total length. Calculating percentages\n",
    "# becomes easier this way\n",
    "df['total_len_seq'] = df.groupby(['sequence'],sort=False)['stop_pos'].transform(max)\n",
    "\n",
    "# calculate percentage coverage for each mapping piece to the contig\n",
    "df['percentage_of_seq'] = df['num_of_nucl'] / df['total_len_seq']\n",
    "\n",
    "\n",
    "# make a new df where only values with coverage not 0 are in\n",
    "df_no0 = df.loc[df['coverage'] != 0]\n",
    "\n",
    "# add up all the percentages for sequences with the same name\n",
    "# reset index to keep column names\n",
    "df_percentages = df_no0.groupby(['sequence'])['percentage_of_seq'].agg('sum').reset_index()\n",
    "\n",
    "# make it actual percentages by multiplying with 100\n",
    "df_percentages['percentage_of_seq'] = df_percentages['percentage_of_seq']*100\n",
    "\n",
    "\n",
    "# Select the cases where the percentage coverage is < 74.99\n",
    "percentage_under_75 = df_percentages['percentage_of_seq'] < 74.99\n",
    "\n",
    "# keep cases where perc coverage < 75, to new csv\n",
    "df_percentages[percentage_under_75].to_csv(sys.argv[2], sep='\\t')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use Bamm to make  a coverage table from all bam files\n",
    "- http://ecogenomics.github.io/BamM/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use bamm to create coverage table\n",
    "bamm parse -c output_file.tsv -b *.bam -m 'tpmean'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter the coverage table based on the 75% lenght coverage parameter \n",
    "- This is done with custom python script\n",
    "- Input is the coverage table and the SPRUCE_contigs_under75.csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! /usr/bin/env python \n",
    "\n",
    "# imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import sys\n",
    "\n",
    "# open the coverage table\n",
    "df = pd.read_csv('coverage_table.tsv',sep='\\t')\n",
    "\n",
    "# open the under 75% coverage list per sample\n",
    "df_under_75= pd.read_csv('under_75_per_sample.csv',sep='|')\n",
    "\n",
    "# make the headers readable, cut from the _S# part\n",
    "df.columns = df.columns.str.split('_L006').str[0] + ''\n",
    "df_under_75.columns = df_under_75.columns.str.split('_L006').str[0] + ''\n",
    "\n",
    "#remove viral contigs with all 0s from the coverage file\n",
    "df_cov = remove_all_zeros(df)\n",
    "\n",
    "# check the lenght of the dataframe and the number of non zero values in each \n",
    "print 'before', len(df_cov), df_cov.astype(bool).sum(axis=0)\n",
    "\n",
    "#Do the function that checks each column(sample) for a coverage under 75%. \n",
    "for col in df_under_75.columns:\n",
    "       df_cov[col] = df_cov[['#contig', col]].apply(lambda x: check_similarity(x['#contig'], df_under_75[col].values[1:], x[col]),axis=1)\n",
    "    \n",
    "#after removing these under 75% coverage ones, remove the columns with all 0 again.\n",
    "df_cov = remove_all_zeros(df_cov)\n",
    "\n",
    "\n",
    "# check the len of df and num of viral contigs with > 75% coverage for each sample\n",
    "print 'after', len(df_cov),  df_cov.astype(bool).sum(axis=0)\n",
    "\n",
    "#print df_cov\n",
    "df_cov.to_csv('filtered_coverage_table.csv', sep='|')\n",
    "\n",
    "\n",
    "def remove_all_zeros(df):\n",
    "\n",
    "    # get a list of columns that could contain zero\n",
    "    all_columns =  list(df.columns)[2:]\n",
    "\n",
    "    # check if all columns contain zero\n",
    "    df[\"all_zeros\"] = (df[all_columns] == 0).all(axis=1)\n",
    "\n",
    "    # remove all that have only zeros\n",
    "    df = df.loc[df[\"all_zeros\"] == False]\n",
    "\n",
    "    # remove all zeros column\n",
    "    df.drop(['all_zeros'], axis=1, inplace=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def check_similarity(contig_name, contig_names_to_check, curent_value):\n",
    "        \n",
    "    if contig_name in contig_names_to_check:\n",
    "        return 0\n",
    "    else: return curent_value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
